{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZfsNFO1End"
      },
      "source": [
        "#### Nomes: David Rodrigues Albuquerque e Ramon Oliveira de Azevedo\n",
        "#### DRE: 120047390 e 120023419\n",
        "#### 7° Período\n",
        "#### Ciência da Computação"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y0iKGfd11G6l"
      },
      "source": [
        "# Projeto de Introdução ao Aprendizado de Máquina - Professor João Carlos Pereira da Silva"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHpBMjT1I6N"
      },
      "source": [
        "# <center>Aplicação de Aprendizado por Reforço - Snake Game</center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GZXAsnLRUdlw"
      },
      "source": [
        "## Sumário\n",
        "\n",
        "*   Apresentação do projeto e objetivo\n",
        "*   O que é aprendizado por reforço?\n",
        "*   Q-Learning\n",
        "*   Q-Table e Equação de Bellman\n",
        "*   Implementação\n",
        "*   Conjunto de Testes\n",
        "*   Análise dos resultados obtidos\n",
        "*   Agradecimentos\n",
        "*   Referências Bibliográficas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nWi22YT1VCz7"
      },
      "source": [
        "### Apresentação do projeto e objetivo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jSENQcyUcGbf"
      },
      "source": [
        "Este trabalho tem como objetivo a realização de estudos envolvendo o campo de Aprendizado de Reforço em Python, sendo o mesmo aplicado em um Snake Game, se caracterizando como a quarta tarefa dada pela disciplina \"Introdução ao Aprendizado de Máquina\", sendo a mesma ministrada pelo professor João Carlos Pereira da Silva. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F14kuGVKLsbJ"
      },
      "source": [
        "### O que é aprendizado por reforço?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O aprendizado por reforço (ou Reinforcement Learning – RL) se caracteriza como um modelo de aprendizado semi-supervisionado em Machine Learning, sendo uma técnica para permitir que um agente tome ações e interaja com um ambiente, a fim de maximizar as recompensas totais. Mas como isso acontece? \n",
        "Nesse tipo de aprendizado, o agente não é informado sobre as ações corretas a serem executadas, mas recebe feedback na forma de recompensas ou punições após realizar uma ação. O objetivo do agente é aprender uma política de ações que maximize a recompensa acumulada ao longo do tempo.\n",
        "\n",
        "Existem três componentes fundamentais no Aprendizado por Reforço: o Agente (agent), o Ambiente (Environment) e a Recompensa (Reward). O agente é responsável por tomar ações com base nas informações do ambiente, enquanto o ambiente é o contexto onde o agente interage. Paralelo a isso, a recompensa é uma medida numérica que o agente recebe após cada ação, indicando o quão boa foi sua escolha. \n",
        "\n",
        "Utilizando um exemplo do nosso cotidiano, podemos pensar na lógica por trás do adestramento de um animal doméstico, como por exemplo, um cachorro. Ao treinarmos, a cada tarefa bem sucedida damos a ele uma recompensa como um carinho ou um biscoito. Porém, em caso de desobediência damos a ele uma espécie de \"punição\" como uma pequena bronca. É com essa linha de raciocínio que a Aprendizagem por Reforço se baseia, se caracterizando como a ciência de tomar decisões consideradas como ótimas usando a experiência como principal fator avaliativo. De maneira visual, podemos demonstrar toda essa interação através da imagem abaixo: \n",
        "\n",
        "![Interação entre Agente e Ambiente](Imagens/Agent_Enviroment.png)\n",
        "\n",
        "Desta maneira, podemos dividir todo esse processo em etapas simples, são elas:\n",
        "\n",
        "*  Observação do ambiente\n",
        "*  Decidir como agir usando alguma estratégia\n",
        "*  Agindo de acordo com a abordagem escolhida\n",
        "*  Receber uma recompensa ou penalidade\n",
        "*  Aprendendo com as experiências e refinando nossa estratégia\n",
        "*  Iterar até que uma estratégia ótima seja encontrada\n",
        "\n",
        "Em termos mais técnicos, o Aprendizado Por Reforço é uma tentativa de modelar uma distribuição de probabilidade complexa de recompensas em relação a um número muito grande de pares de ação de estado. Esse é um dos motivos pelos quais o Aprendizado Por Reforço é combinado com, digamos, um processo de decisão de Markov (MDP – Markov Decision Process), pois o mesmo serve justamente para modelar e solucionar situações onde uma sequência de ações será executada em um ambiente onde não há certeza sobre o estado atual e o resultado da ação aplicada sobre esse estado. Em linhas gerais, esse tipo de raciocínio se assemelha bastante com o problema que fez com que Stan Ulam inventasse o método de Monte Carlo, podendo gerar certas confusões, já que o mesmo busca estimar resultados através da geração de números aleatórios e da repetição de experimentos, sendo especialmente útil em problemas complexos em que as soluções analíticas não são práticas ou não estão disponíveis.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dando continuidade, existem basicamente 2 tipos principais de algoritmos de RL. Eles são baseados em modelo e sem modelo.\n",
        "\n",
        "*  Algoritmo sem modelo - É um algoritmo que estima a política ótima sem usar ou estimar a dinâmica (funções de transição e recompensa) do ambiente. Desta forma, o algoritmo é capaz de predizer os estados futuros a partir do estado atual. Um exemplo disto é um jogo de xadrez, onde é possível explorar os movimentos futuros com base no estado atual do jogo. \n",
        "*  Algoritmo baseado em modelo - É um algoritmo que usa a função de transição (e a função de recompensa) para estimar a política ideal\n",
        "\n",
        "\n",
        "![Hierarquia - Aprendizado pro Reforço](Imagens/Hierarquia_RL.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O Q-Learning é um algoritmo de aprendizado por reforço da categoria de algoritmos Model-Free. Esse algoritmo é dito value-based, isto é, ele atualizar a sua value-function através de uma equação: a equação de Bellman. \n",
        "\n",
        "De modo geral, ele é um método de aprendizado por reforço que ensina um agente de aprendizagem a realizar uma tarefa, recompensando o bom comportamento e punindo o mau comportamento. Em Snake, por exemplo, aproximar-se da comida é bom. Sair da tela é ruim. A cada ponto do jogo, o agente escolherá a ação com maior recompensa esperada.\n",
        "\n",
        "Definição:\n",
        "\n",
        "- Q*(s,a): Valor esperado tomando a ação *a* no espaço *s* e seguindo a optimal policy.\n",
        "- O algoritmo irá usar a diferença temporal: estimar o valor de Q*(s,a) baseado em um agente aprendendo dentro do environment através de iterações/épocas sem nenhum conhecimento prévio.\n",
        "- O agente mantém uma tabela **Q[s,a]** em que S é uma conjunto de estados e A um conjunto de ações.\n",
        "- **Q[s,a]** representa a estimativa no instante de tempo *t* de Q*(s,a)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-Table e Equação de Bellman"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma Q-Table, também conhecida como tabela-Q, é uma estrutura de dados utilizada no algoritmo de Q-Learning para armazenar os valores Q para cada par estado-ação. Cada entrada na tabela representa um estado específico e uma ação possível a ser tomada nesse estado. O valor Q em uma determinada entrada indica a \"qualidade\" ou a recompensa esperada ao realizar a ação naquele estado.\n",
        "\n",
        "A Q-Tabela é inicializada com valores arbitrários e é atualizada iterativamente à medida que o agente interage com o ambiente. Através da atualização dos valores Q, a Q-Table gradualmente converge para os valores ótimos que representam a política de ações a serem tomadas em cada estado. Podemos notar um exemplo disso através da imagem abaixo, que retrata uma Q-Table após N execuções:\n",
        "\n",
        "![Q-Table](Imagens/QTable_Execucoes.webp)\n",
        "\n",
        "\n",
        "\n",
        "Equação de Bellman:\n",
        "\n",
        "A equação de Bellman é uma fórmula que descreve como os valores Q podem ser atualizados na Q-Tabela durante o processo de aprendizado por reforço. Ela se baseia no princípio de otimalidade de Bellman, que afirma que um valor Q ótimo para um estado é igual à recompensa imediata dessa ação mais o valor Q máximo esperado do próximo estado.\n",
        "\n",
        "![Equação de Bellman](Imagens/Bellman.png)\n",
        "\n",
        "\n",
        "Nessa equação:\n",
        "\n",
        "Q(s, a) representa o valor Q atual para o par estado-ação (s, a).\n",
        "α (alfa) é a taxa de aprendizado, um valor entre 0 e 1 que controla o quão rapidamente os novos aprendizados substituem os valores antigos na tabela.\n",
        "R é a recompensa imediata recebida após a transição do estado s para o estado s' ao realizar a ação a.\n",
        "γ (gama) é o fator de desconto, um valor entre 0 e 1 que pondera a importância das recompensas futuras em relação às recompensas imediatas.\n",
        "max(Q(s', a')) é o valor Q máximo esperado para todas as ações possíveis no próximo estado s'.\n",
        "A equação de Bellman permite que o agente atualize o valor Q para um par estado-ação com base na recompensa recebida e no valor Q máximo esperado do próximo estado. Essa atualização é realizada de forma iterativa para cada transição de estado, permitindo que o agente refine gradualmente sua estimativa dos valores Q ótimos.\n",
        "\n",
        "Resumidamente, a Q-Table é uma tabela que armazena os valores Q para cada par estado-ação, e a equação de Bellman é uma fórmula usada para atualizar os valores Q na Q-Table, levando em consideração as recompensas imediatas e as recompensas futuras esperadas. Essas duas componentes são fundamentais no algoritmo de Q-Learning para aprender uma política ótima de ações em um ambiente de aprendizado por reforço."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0GcpFQvE1Li1"
      },
      "source": [
        "### Implementação"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tgsCqjP11Op1"
      },
      "source": [
        "##### Começando do básico, vamos importar as bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uWf-V-Zf1Vld"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Como esse trabalho servirá como portfólio pessoal, vale deixar explícito as versões que aqui serão trabalhadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
            "Pygame: 2.4.0\n",
            "Seaborn: 0.11.2\n"
          ]
        }
      ],
      "source": [
        "print('Python: {}'.format(sys.version))\n",
        "print('Pygame: {}'.format(pygame.__version__))\n",
        "print('Seaborn: {}'.format(sns.__version__))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, temos as constantes que serão utilizadas no decorrer de nosso código. Elas se referem ao tamanho do ambiente que será utilizado ao nosso modelo, ou seja, o tamanho do nosso grid, que no caso será $20X20$. Além disso, temos outras definições técnicas, como "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo as constantes\n",
        "GRID_SIZE = 20\n",
        "GRID_WIDTH = 20\n",
        "GRID_HEIGHT = 20\n",
        "WINDOW_SIZE = (GRID_WIDTH * GRID_SIZE, GRID_HEIGHT * GRID_SIZE)\n",
        "FPS = 10\n",
        "BLACK = (0, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (255, 0, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, temos a classe responsável pelo \"controle\" da Cobra no decorrer do jogo e por realizar todo o processo envolvendo o Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningSnake:\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.q_table = np.zeros((self.width * self.height, self.width * self.height ,  4))\n",
        "        self.epsilon = 1.0\n",
        "        self.min_epsilon = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.9\n",
        "        self.reset()\n",
        "\n",
        "    def generate_apple(self):\n",
        "        while True:\n",
        "            apple = (random.randint(0, self.width - 1),\n",
        "                     random.randint(0, self.height - 1))\n",
        "            if apple not in self.snake:\n",
        "                return apple\n",
        "\n",
        "    def change_direction(self, direction):\n",
        "        if direction == 'up' and self.direction != 'down':\n",
        "            self.direction = 'up'\n",
        "        elif direction == 'down' and self.direction != 'up':\n",
        "            self.direction = 'down'\n",
        "        elif direction == 'left' and self.direction != 'right':\n",
        "            self.direction = 'left'\n",
        "        elif direction == 'right' and self.direction != 'left':\n",
        "            self.direction = 'right'\n",
        "\n",
        "    def move(self):\n",
        "        head = self.snake[0]\n",
        "        x, y = head\n",
        "\n",
        "        if self.direction == 'up':\n",
        "            y -= 1\n",
        "        elif self.direction == 'down':\n",
        "            y += 1\n",
        "        elif self.direction == 'left':\n",
        "            x -= 1\n",
        "        elif self.direction == 'right':\n",
        "            x += 1\n",
        "\n",
        "        if x < 0 or x >= self.width or y < 0 or y >= self.height or (x, y) in self.snake[1:]:\n",
        "            self.game_over = True\n",
        "            return True, False\n",
        "\n",
        "        self.snake.insert(0, (x, y))\n",
        "\n",
        "        if self.snake[0] == self.apple:\n",
        "            self.score += 1\n",
        "            self.apple = self.generate_apple()\n",
        "            return False, True\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "            return False, False\n",
        "\n",
        "    def get_state_index(self):\n",
        "        head_pos = self.snake[0]\n",
        "        apple_pos = self.apple\n",
        "        return head_pos[0] * self.height + head_pos[1], apple_pos[0] * self.height + apple_pos[1]\n",
        "\n",
        "    def select_action(self, state_index):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, 3)  # Choose random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def update_q_table(self, state_index, action_index, reward, next_state_index):\n",
        "        old_value = self.q_table[state_index][action_index]\n",
        "        next_max = np.max(self.q_table[next_state_index])\n",
        "        new_value = old_value + self.alpha * \\\n",
        "            (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table[state_index][action_index] = new_value\n",
        "\n",
        "    def reset(self):\n",
        "        self.snake = [(self.width // 2, self.height // 2)]\n",
        "        self.apple = self.generate_apple()\n",
        "        self.direction = random.choice(['up', 'down', 'left', 'right'])\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        # Dados estatísticos para gerar os gráficos\n",
        "        rewards = []\n",
        "        scores = []\n",
        "        average_scores = []\n",
        "        epsilons = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            self.reset()\n",
        "            done = False\n",
        "            rounds_without_apple = 0\n",
        "            max_rounds_without_apple = 200\n",
        "\n",
        "            while not done:\n",
        "                # Escolha na Q-Table e movimento da cobra\n",
        "                state_index = self.get_state_index()\n",
        "                action_index = self.select_action(state_index)\n",
        "                self.change_direction(\n",
        "                    ['up', 'down', 'left', 'right'][action_index])\n",
        "                done, ate_apple = self.move()\n",
        "\n",
        "                # Função de recompensa + checa se a cobra está em loop sem comer maçã\n",
        "                reward = 0\n",
        "                # if rounds_without_apple >= max_rounds_without_apple:\n",
        "                #     done = True\n",
        "                if done:\n",
        "                    reward = -10\n",
        "                elif ate_apple:\n",
        "                    reward = 10\n",
        "                    rounds_without_apple = 0\n",
        "                else:\n",
        "                    reward = -1\n",
        "                    rounds_without_apple += 1\n",
        "                \n",
        "\n",
        "                # Atualiza Q-Table\n",
        "                next_state_index = self.get_state_index()\n",
        "                self.update_q_table(state_index, action_index,\n",
        "                                    reward, next_state_index)\n",
        "\n",
        "            # Atualiza taxa de exploração\n",
        "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            # Resolução dos dados estatísticos\n",
        "            rewards.append(self.score)\n",
        "            scores.append(self.score)\n",
        "            average_score = np.mean(scores[-100:])\n",
        "            average_score = 1\n",
        "            average_scores.append(average_score)\n",
        "            epsilons.append(self.epsilon)\n",
        "\n",
        "            print(\n",
        "                f\"Episode: {episode + 1}/{num_episodes}, Score: {self.score}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Treinamento levou {elapsed_time:.2f} segundos\")\n",
        "\n",
        "        return rewards, scores, average_scores, epsilons\n",
        "\n",
        "    def render(self, screen):\n",
        "        screen.fill(BLACK)\n",
        "        for segment in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                screen, GREEN, (segment[0] * GRID_SIZE, segment[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "        pygame.draw.rect(\n",
        "            screen, RED, (self.apple[0] * GRID_SIZE, self.apple[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "        pygame.display.flip()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos perceber, a classe abaixo possui um caráter mais simplista com relação a anterior. Isso se deve ao fato dela ser responsável pelo jogo propriamente dito, ou seja, por rodar a interface gráfica criada. Dessa forma, Classe responsável pelo jogo propriamente dito"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PygameSnakeGame:\n",
        "    def __init__(self, q_learning_snake):\n",
        "        self.snake_game = q_learning_snake\n",
        "\n",
        "    def draw_parameters(self, screen):\n",
        "        font = pygame.font.Font(None, 24)\n",
        "        text_margin = 10\n",
        "\n",
        "        score_text = font.render(\n",
        "            f\"Score: {self.snake_game.score}\", True, (255, 255, 255))\n",
        "        screen.blit(\n",
        "            score_text, (WINDOW_SIZE[0] - score_text.get_width() - text_margin, text_margin))\n",
        "\n",
        "        # Adicione outros parâmetros aqui, se desejar\n",
        "\n",
        "    def play(self):\n",
        "        # Inicializando o Pygame e executando o jogo com o modelo treinado\n",
        "        pygame.init()\n",
        "        pygame.display.set_caption(\"Snake Q-Learning\")\n",
        "        screen = pygame.display.set_mode(WINDOW_SIZE)\n",
        "        clock = pygame.time.Clock()\n",
        "        \n",
        "        running = True\n",
        "\n",
        "        while running:\n",
        "            self.snake_game.reset()  # Resetar o jogo antes de começar a jogar\n",
        "\n",
        "            rounds_without_apple = 0\n",
        "            max_rounds_without_apple = 200\n",
        "\n",
        "            while not self.snake_game.game_over:\n",
        "                # Checa se o jogo foi encerrado\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        self.game_over = True\n",
        "                        running = False\n",
        "                \n",
        "                # Escolha na Q-Table e movimento da cobra\n",
        "                state_index = self.snake_game.get_state_index()\n",
        "                action_index = np.argmax(self.snake_game.q_table[state_index])\n",
        "                self.snake_game.change_direction(\n",
        "                    ['up', 'down', 'left', 'right'][action_index])\n",
        "                _, ate_apple = self.snake_game.move()\n",
        "\n",
        "                # Checa se a cobra está em loop sem comer maçãs\n",
        "                if ate_apple:\n",
        "                    rounds_without_apple = 0\n",
        "                else:\n",
        "                    rounds_without_apple += 1\n",
        "                if rounds_without_apple >= max_rounds_without_apple:\n",
        "                    self.snake_game.game_over = True\n",
        "\n",
        "                # Renderizar o jogo na janela\n",
        "                screen.fill(BLACK)\n",
        "                self.draw_parameters(screen)\n",
        "                for segment in self.snake_game.snake:\n",
        "                    pygame.draw.rect(\n",
        "                        screen, GREEN, (segment[0] * GRID_SIZE, segment[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "                pygame.draw.rect(\n",
        "                    screen, RED, (self.snake_game.apple[0] * GRID_SIZE, self.snake_game.apple[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "                pygame.display.flip()\n",
        "                clock.tick(FPS)\n",
        "\n",
        "            print(f\"Game Over! Score: {self.snake_game.score}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        pygame.quit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_plots(rewards, scores, average_scores, epsilons):\n",
        "    # Gráfico de recompensa ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(rewards) + 1), rewards)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Recompensa\")\n",
        "    plt.title(\"Recompensa ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de pontuação ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(scores) + 1), scores)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Pontuação\")\n",
        "    plt.title(\"Pontuação ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de pontuação média ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(average_scores) + 1), average_scores)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Pontuação Média\")\n",
        "    plt.title(\"Pontuação Média ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de epsilon ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(epsilons) + 1), epsilons)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    plt.title(\"Epsilon ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Salvar os gráficos em um arquivo (opcional)\n",
        "    # plt.savefig(\"q_learning_plots.png\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de Testes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 1:\n",
        "- epsilon: 1.0\n",
        "- min_epsilon: 0.01\n",
        "- min_epsilon_decay: 0.995\n",
        "- alpha: 0.1\n",
        "- gamma: 0.9\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor alto para epsilon (1.0), o que significa que a exploração é inicialmente priorizada. Com o tempo, a probabilidade de explorar diminui gradualmente devido ao fator de decaimento (min_epsilon_decay = 0.995), chegando ao valor mínimo de 0.01 (min_epsilon). O alpha é definido como 0.1, permitindo um equilíbrio entre a importância dos novos dados e o conhecimento acumulado (taxa de aprendizado). O gamma é definido como 0.9 para dar um desconto alto para recompensas futuras, incentivando a maximização de recompensas ao longo do tempo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 2:\n",
        "- epsilon: 0.5\n",
        "- min_epsilon: 0.001\n",
        "- min_epsilon_decay: 0.999\n",
        "- alpha: 0.2\n",
        "- gamma: 0.95\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor moderado para epsilon (0.5), priorizando a exploração, e diminuímos ainda mais lentamente a probabilidade de explorar (min_epsilon_decay = 0.999) até o valor mínimo de 0.001 (min_epsilon). O alpha é definido como 0.2, permitindo um pouco mais de importância para os novos dados na atualização da tabela Q. O gamma é definido como 0.95 para dar um desconto um pouco mais alto para recompensas futuras."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Conjunto de Teste 3:\n",
        "- epsilon: 0.8\n",
        "- min_epsilon: 0.05\n",
        "- min_epsilon_decay: 0.99\n",
        "- alpha: 0.05\n",
        "- gamma: 0.8\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor relativamente alto para epsilon (0.8), priorizando a exploração no início, e diminuímos a probabilidade de explorar um pouco mais rapidamente (min_epsilon_decay = 0.99) até o valor mínimo de 0.05 (min_epsilon). O alpha é definido como 0.05, dando menos importância para os novos dados e enfatizando o conhecimento acumulado. O gamma é definido como 0.8 para dar um desconto moderado para recompensas futuras."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Conjunto de Teste 4:\n",
        "- epsilon: 0.2\n",
        "- min_epsilon: 0.1\n",
        "- min_epsilon_decay: 0.995\n",
        "- alpha: 0.3\n",
        "- gamma: 0.99\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor relativamente baixo para epsilon (0.2), priorizando a exploração no início, e diminuímos a probabilidade de explorar gradualmente (min_epsilon_decay = 0.995) até o valor mínimo de 0.1 (min_epsilon). O alpha é definido como 0.3, dando mais importância para os novos dados na atualização da tabela Q. O gamma é definido como 0.99 para dar um desconto alto para recompensas futuras, incentivando a maximização de recompensas ao longo do tempo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 5:\n",
        "- epsilon: 0.1\n",
        "- min_epsilon: 0.001\n",
        "- min_epsilon_decay: 0.9999\n",
        "- alpha: 0.1\n",
        "- gamma: 0.95\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor baixo para epsilon (0.1), priorizando a exploração no início, e diminuímos muito lentamente a probabilidade de explorar (min_epsilon_decay = 0.9999) até o valor mínimo de 0.001 (min_epsilon). O alpha é definido como 0.1, dando um equilíbrio entre a importância dos novos dados e o conhecimento acumulado na atualização da tabela Q. O gamma é definido como 0.95 para dar um desconto um pouco mais alto para recompensas futuras."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos resultados obtidos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxQT1vP2U4U"
      },
      "source": [
        "### Agradecimentos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oej9nzPsS6l"
      },
      "source": [
        "Gostaria de utilizar esse espaço para ressaltar meus agradecimentos aos seguintes professores:\n",
        "\n",
        "*   Hugo Tremonte de Carvalho\n",
        "*   João Antônio Recio da Paixão\n",
        "*   João Carlos Pereira da Silva\n",
        "\n",
        "Por fim, espero ter conseguido articular bem os conceitos, pois os mesmos não são fáceis. Dessa forma, venho por meio deste terminar com uma breve citação:\n",
        "\n",
        "# ***“A esperança é uma coisa boa, talvez a melhor de todas, e nada que é bom, deve morrer” - King, Stephen***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMHuTboOfoI"
      },
      "source": [
        "# Referências Bibliográficas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ20lrJjOf8s"
      },
      "source": [
        "https://www.youtube.com/watch?v=je0DdS0oIZk\n",
        "\n",
        "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/\n",
        "\n",
        "https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c\n",
        "\n",
        "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#\n",
        "\n",
        "https://www.deeplearningbook.com.br/componentes-do-aprendizado-por-reforco-reinforcement-learning/\n",
        "\n",
        "https://www.deeplearningbook.com.br/distribuicoes-de-probabilidade-redes-neurais-e-reinforcement-learning/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
