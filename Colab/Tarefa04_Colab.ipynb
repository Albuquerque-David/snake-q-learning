{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZfsNFO1End"
      },
      "source": [
        "#### Nomes: David Rodrigues Albuquerque e Ramon Oliveira de Azevedo\n",
        "#### DRE: 120047390 e 120023419\n",
        "#### 7° Período\n",
        "#### Ciência da Computação"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y0iKGfd11G6l"
      },
      "source": [
        "# Projeto de Introdução ao Aprendizado de Máquina - Professor João Carlos Pereira da Silva"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHpBMjT1I6N"
      },
      "source": [
        "# <center>Aplicação de Aprendizado por Reforço - Snake Game</center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GZXAsnLRUdlw"
      },
      "source": [
        "## Sumário\n",
        "\n",
        "*   Apresentação do projeto e objetivo\n",
        "*   O que é aprendizado por reforço?\n",
        "*   Q-Learning\n",
        "*   Q-Table\n",
        "*   Equação de Bellman\n",
        "*   Implementação\n",
        "*   Conjunto de Testes\n",
        "*   Análise dos resultados obtidos\n",
        "*   Treinando o nosso modelo\n",
        "*   Agradecimentos\n",
        "*   Referências Bibliográficas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nWi22YT1VCz7"
      },
      "source": [
        "### Apresentação do projeto e objetivo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jSENQcyUcGbf"
      },
      "source": [
        "Este trabalho tem como objetivo a realização de estudos envolvendo o campo de Aprendizado de Reforço em Python, sendo o mesmo aplicado em um Snake Game, se caracterizando como a quarta tarefa dada pela disciplina \"Introdução ao Aprendizado de Máquina\", sendo a mesma ministrada pelo professor João Carlos Pereira da Silva. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F14kuGVKLsbJ"
      },
      "source": [
        "### O que é aprendizado por reforço?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O aprendizado por reforço (ou Reinforcement Learning – RL) se caracteriza como um modelo de aprendizado semi-supervisionado em Machine Learning, sendo uma técnica para permitir que um agente tome ações e interaja com um ambiente, a fim de maximizar as recompensas totais. Mas como isso acontece? \n",
        "Nesse tipo de aprendizado, o agente não é informado sobre as ações corretas a serem executadas, mas recebe feedback na forma de recompensas ou punições após realizar uma ação. O objetivo do agente é aprender uma política de ações que maximize a recompensa acumulada ao longo do tempo.\n",
        "\n",
        "Existem três componentes fundamentais no Aprendizado por Reforço: o Agente (agent), o Ambiente (Environment) e a Recompensa (Reward). O agente é responsável por tomar ações com base nas informações do ambiente, enquanto o ambiente é o contexto onde o agente interage. Paralelo a isso, a recompensa é uma medida numérica que o agente recebe após cada ação, indicando o quão boa foi sua escolha. \n",
        "\n",
        "Utilizando um exemplo do nosso cotidiano, podemos pensar na lógica por trás do adestramento de um animal doméstico, como por exemplo, um cachorro. Ao treinarmos, a cada tarefa bem sucedida damos a ele uma recompensa como um carinho ou um biscoito. Porém, em caso de desobediência damos a ele uma espécie de \"punição\" como uma pequena bronca. É com essa linha de raciocínio que a Aprendizagem por Reforço se baseia, se caracterizando como a ciência de tomar decisões consideradas como ótimas usando a experiência como principal fator avaliativo. De maneira visual, podemos demonstrar toda essa interação através da imagem abaixo: \n",
        "\n",
        "![Interação entre Agente e Ambiente](Imagens/Agent_Enviroment.png)\n",
        "\n",
        "Desta maneira, podemos dividir todo esse processo em etapas simples, são elas:\n",
        "\n",
        "*  Observação do ambiente\n",
        "*  Decidir como agir usando alguma estratégia\n",
        "*  Agindo de acordo com a abordagem escolhida\n",
        "*  Receber uma recompensa ou penalidade\n",
        "*  Aprendendo com as experiências e refinando nossa estratégia\n",
        "*  Iterar até que uma estratégia ótima seja encontrada\n",
        "\n",
        "Em termos mais técnicos, o Aprendizado Por Reforço é uma tentativa de modelar uma distribuição de probabilidade complexa de recompensas em relação a um número muito grande de pares de ação de estado. Esse é um dos motivos pelos quais o Aprendizado Por Reforço é combinado com, digamos, um processo de decisão de Markov (MDP – Markov Decision Process), pois o mesmo serve justamente para modelar e solucionar situações onde uma sequência de ações será executada em um ambiente onde não há certeza sobre o estado atual e o resultado da ação aplicada sobre esse estado. Em linhas gerais, esse tipo de raciocínio se assemelha bastante com o problema que fez com que Stan Ulam inventasse o método de Monte Carlo, podendo gerar certas confusões, já que o mesmo busca estimar resultados através da geração de números aleatórios e da repetição de experimentos, sendo especialmente útil em problemas complexos em que as soluções analíticas não são práticas ou não estão disponíveis.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dando continuidade, existem basicamente 2 tipos principais de algoritmos de RL. Eles são baseados em modelo e sem modelo.\n",
        "\n",
        "*  Algoritmo sem modelo - É um algoritmo que estima a política ótima sem usar ou estimar a dinâmica (funções de transição e recompensa) do ambiente. Desta forma, o algoritmo é capaz de predizer os estados futuros a partir do estado atual. Um exemplo disto é um jogo de xadrez, onde é possível explorar os movimentos futuros com base no estado atual do jogo. \n",
        "*  Algoritmo baseado em modelo - É um algoritmo que usa a função de transição (e a função de recompensa) para estimar a política ideal\n",
        "\n",
        "\n",
        "![Hierarquia - Aprendizado pro Reforço](Imagens/Hierarquia_RL.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O Q-Learning é um algoritmo de aprendizado por reforço da categoria de algoritmos Model-Free. Esse algoritmo é dito value-based, isto é, ele atualizar a sua value-function através de uma equação: a equação de Bellman. \n",
        "\n",
        "De modo geral, ele é um método de aprendizado por reforço que ensina um agente de aprendizagem a realizar uma tarefa, recompensando o bom comportamento e punindo o mau comportamento. Em Snake, por exemplo, aproximar-se da comida é bom. Sair da tela é ruim. A cada ponto do jogo, o agente escolherá a ação com maior recompensa esperada.\n",
        "\n",
        "Definição:\n",
        "\n",
        "- Q*(s,a): Valor esperado tomando a ação *a* no espaço *s* e seguindo a optimal policy.\n",
        "- O algoritmo irá usar a diferença temporal: estimar o valor de Q*(s,a) baseado em um agente aprendendo dentro do environment através de iterações/épocas sem nenhum conhecimento prévio.\n",
        "- O agente mantém uma tabela **Q[s,a]** em que S é uma conjunto de estados e A um conjunto de ações.\n",
        "- **Q[s,a]** representa a estimativa no instante de tempo *t* de Q*(s,a)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-Table"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma Q-Table, também conhecida como tabela-Q, é uma estrutura de dados utilizada no algoritmo de Q-Learning para armazenar os valores Q para cada par estado-ação. Cada entrada na tabela representa um estado específico e uma ação possível a ser tomada nesse estado. O valor Q em uma determinada entrada indica a \"qualidade\" ou a recompensa esperada ao realizar a ação naquele estado.\n",
        "\n",
        "A Q-Tabela é inicializada com valores arbitrários e é atualizada iterativamente à medida que o agente interage com o ambiente. Através da atualização dos valores Q, a Q-Table gradualmente converge para os valores ótimos que representam a política de ações a serem tomadas em cada estado. Podemos notar um exemplo disso através da imagem abaixo, que retrata uma Q-Table após N execuções:\n",
        "\n",
        "![Q-Table](Imagens/QTable_Execucoes.webp)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Equação de Bellman\n",
        "\n",
        "A equação de Bellman é uma fórmula que descreve como os valores Q podem ser atualizados na Q-Tabela durante o processo de aprendizado por reforço. Ela se baseia no princípio de otimalidade de Bellman, que afirma que um valor Q ótimo para um estado é igual à recompensa imediata dessa ação mais o valor Q máximo esperado do próximo estado.\n",
        "\n",
        "![Equação de Bellman](Imagens/Bellman.png)\n",
        "\n",
        "\n",
        "Nessa equação:\n",
        "\n",
        "Q(s, a) representa o valor Q atual para o par estado-ação (s, a).\n",
        "α (alfa) é a taxa de aprendizado, um valor entre 0 e 1 que controla o quão rapidamente os novos aprendizados substituem os valores antigos na tabela.\n",
        "R é a recompensa imediata recebida após a transição do estado s para o estado s' ao realizar a ação a.\n",
        "γ (gama) é o fator de desconto, um valor entre 0 e 1 que pondera a importância das recompensas futuras em relação às recompensas imediatas.\n",
        "max(Q(s', a')) é o valor Q máximo esperado para todas as ações possíveis no próximo estado s'.\n",
        "A equação de Bellman permite que o agente atualize o valor Q para um par estado-ação com base na recompensa recebida e no valor Q máximo esperado do próximo estado. Essa atualização é realizada de forma iterativa para cada transição de estado, permitindo que o agente refine gradualmente sua estimativa dos valores Q ótimos.\n",
        "\n",
        "Resumidamente, a Q-Table é uma tabela que armazena os valores Q para cada par estado-ação, e a equação de Bellman é uma fórmula usada para atualizar os valores Q na Q-Table, levando em consideração as recompensas imediatas e as recompensas futuras esperadas. Essas duas componentes são fundamentais no algoritmo de Q-Learning para aprender uma política ótima de ações em um ambiente de aprendizado por reforço."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0GcpFQvE1Li1"
      },
      "source": [
        "### Implementação"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tgsCqjP11Op1"
      },
      "source": [
        "##### Começando do básico, vamos importar as bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "uWf-V-Zf1Vld"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Como esse trabalho servirá como portfólio pessoal, vale deixar explícito as versões que aqui serão trabalhadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
            "Pygame: 2.4.0\n",
            "Seaborn: 0.11.2\n"
          ]
        }
      ],
      "source": [
        "print('Python: {}'.format(sys.version))\n",
        "print('Pygame: {}'.format(pygame.__version__))\n",
        "print('Seaborn: {}'.format(sns.__version__))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, temos as constantes que serão utilizadas no decorrer de nosso código. Elas se referem ao tamanho do ambiente que será utilizado ao nosso modelo, ou seja, o tamanho do nosso grid, que no caso será $20X20$. Além disso, temos outras definições técnicas, como as cores que serão aplicadas em nossa interface gráfica, a taxa de Frames Por Segundo (FPS) e o tamanho da Janela (Window) de nossa aplicação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo as constantes\n",
        "GRID_SIZE = 20\n",
        "GRID_WIDTH = 20\n",
        "GRID_HEIGHT = 20\n",
        "WINDOW_SIZE = (GRID_WIDTH * GRID_SIZE, GRID_HEIGHT * GRID_SIZE)\n",
        "FPS = 10\n",
        "BLACK = (0, 0, 0)\n",
        "GREEN = (0, 255, 0)\n",
        "RED = (255, 0, 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, encontra-se a função principal desse Colab, na qual será responsável por realizar todas as nossas futuras experiências. Ela gerencia o ambiente do jogo e o processo de treinamento do agente. Ela possui atributos como largura e altura do tabuleiro (que fora definido anteriormente nas constantes), a Q-Table (que armazena os valores de Q para cada estado e ação possíveis), parâmetros como epsilon (que controla a taxa de exploração versus explotação), taxa de aprendizado, fator de desconto, entre outros.\n",
        "\n",
        "O jogo é implementado através de funções como generate_apple (que gera a posição da maçã), change_direction (que permite ao jogador mudar a direção da cobra), move (que move a cobra na direção escolhida e verifica se houve colisão com as bordas do tabuleiro ou com o próprio corpo), entre outras.\n",
        "\n",
        "O processo de treinamento é realizado pela função train, que executa o jogo em vários episódios, atualiza a tabela Q com base nas recompensas obtidas e controla a taxa de exploração. Durante o treinamento, são coletados dados estatísticos, como as recompensas e pontuações obtidas em cada episódio.\n",
        "\n",
        "Por fim, a função render é responsável por exibir visualmente o jogo na tela, representando a cobra como segmentos verdes e a maçã como um quadrado vermelho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningSnake:\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.q_table = np.zeros((self.width * self.height, self.width * self.height ,  4))\n",
        "        self.epsilon = 1.0\n",
        "        self.min_epsilon = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.9\n",
        "        self.reset()\n",
        "\n",
        "    def generate_apple(self):\n",
        "        while True:\n",
        "            apple = (random.randint(0, self.width - 1),\n",
        "                     random.randint(0, self.height - 1))\n",
        "            if apple not in self.snake:\n",
        "                return apple\n",
        "\n",
        "    def change_direction(self, direction):\n",
        "        if direction == 'up' and self.direction != 'down':\n",
        "            self.direction = 'up'\n",
        "        elif direction == 'down' and self.direction != 'up':\n",
        "            self.direction = 'down'\n",
        "        elif direction == 'left' and self.direction != 'right':\n",
        "            self.direction = 'left'\n",
        "        elif direction == 'right' and self.direction != 'left':\n",
        "            self.direction = 'right'\n",
        "\n",
        "    def move(self):\n",
        "        head = self.snake[0]\n",
        "        x, y = head\n",
        "\n",
        "        if self.direction == 'up':\n",
        "            y -= 1\n",
        "        elif self.direction == 'down':\n",
        "            y += 1\n",
        "        elif self.direction == 'left':\n",
        "            x -= 1\n",
        "        elif self.direction == 'right':\n",
        "            x += 1\n",
        "\n",
        "        if x < 0 or x >= self.width or y < 0 or y >= self.height or (x, y) in self.snake[1:]:\n",
        "            self.game_over = True\n",
        "            return True, False\n",
        "\n",
        "        self.snake.insert(0, (x, y))\n",
        "\n",
        "        if self.snake[0] == self.apple:\n",
        "            self.score += 1\n",
        "            self.apple = self.generate_apple()\n",
        "            return False, True\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "            return False, False\n",
        "\n",
        "    def get_state_index(self):\n",
        "        head_pos = self.snake[0]\n",
        "        apple_pos = self.apple\n",
        "        return head_pos[0] * self.height + head_pos[1], apple_pos[0] * self.height + apple_pos[1]\n",
        "\n",
        "    def select_action(self, state_index):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, 3)  # Choose random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def update_q_table(self, state_index, action_index, reward, next_state_index):\n",
        "        old_value = self.q_table[state_index][action_index]\n",
        "        next_max = np.max(self.q_table[next_state_index])\n",
        "        new_value = old_value + self.alpha * \\\n",
        "            (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table[state_index][action_index] = new_value\n",
        "\n",
        "    def reset(self):\n",
        "        self.snake = [(self.width // 2, self.height // 2)]\n",
        "        self.apple = self.generate_apple()\n",
        "        self.direction = random.choice(['up', 'down', 'left', 'right'])\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        # Dados estatísticos para gerar os gráficos\n",
        "        rewards = []\n",
        "        scores = []\n",
        "        average_scores = []\n",
        "        epsilons = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            self.reset()\n",
        "            done = False\n",
        "            rounds_without_apple = 0\n",
        "            max_rounds_without_apple = 200\n",
        "\n",
        "            while not done:\n",
        "                # Escolha na Q-Table e movimento da cobra\n",
        "                state_index = self.get_state_index()\n",
        "                action_index = self.select_action(state_index)\n",
        "                self.change_direction(\n",
        "                    ['up', 'down', 'left', 'right'][action_index])\n",
        "                done, ate_apple = self.move()\n",
        "\n",
        "                # Função de recompensa + checa se a cobra está em loop sem comer maçã\n",
        "                reward = 0\n",
        "                # if rounds_without_apple >= max_rounds_without_apple:\n",
        "                #     done = True\n",
        "                if done:\n",
        "                    reward = -10\n",
        "                elif ate_apple:\n",
        "                    reward = 10\n",
        "                    rounds_without_apple = 0\n",
        "                else:\n",
        "                    reward = -1\n",
        "                    rounds_without_apple += 1\n",
        "                \n",
        "\n",
        "                # Atualiza Q-Table\n",
        "                next_state_index = self.get_state_index()\n",
        "                self.update_q_table(state_index, action_index,\n",
        "                                    reward, next_state_index)\n",
        "\n",
        "            # Atualiza taxa de exploração\n",
        "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            # Resolução dos dados estatísticos\n",
        "            rewards.append(self.score)\n",
        "            scores.append(self.score)\n",
        "            average_score = np.mean(scores[-100:])\n",
        "            average_score = 1\n",
        "            average_scores.append(average_score)\n",
        "            epsilons.append(self.epsilon)\n",
        "\n",
        "            print(\n",
        "                f\"Episode: {episode + 1}/{num_episodes}, Score: {self.score}\")\n",
        "        \n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Treinamento levou {elapsed_time:.2f} segundos\")\n",
        "\n",
        "        return rewards, scores, average_scores, epsilons\n",
        "\n",
        "    def render(self, screen):\n",
        "        screen.fill(BLACK)\n",
        "        for segment in self.snake:\n",
        "            pygame.draw.rect(\n",
        "                screen, GREEN, (segment[0] * GRID_SIZE, segment[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "        pygame.draw.rect(\n",
        "            screen, RED, (self.apple[0] * GRID_SIZE, self.apple[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "        pygame.display.flip()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos perceber, a classe abaixo possui um caráter mais simplista com relação a anterior. Isso se deve ao fato dela ser responsável pelo jogo propriamente dito, ou seja, por rodar a interface gráfica criada. Dando início ao detalhadamento, o método play é o ponto de entrada do jogo. Ele configura a janela do jogo e inicia o loop principal. Dentro desse loop, o jogo é reiniciado e, em seguida, a cada rodada, a direção da cobra é escolhida com base nas informações do modelo de aprendizado por reforço. A cobra se move de acordo com a direção escolhida e o jogo verifica se ocorreu alguma colisão ou se a cobra comeu uma maçã.\n",
        "\n",
        "Durante o jogo, a interface gráfica é atualizada para mostrar a posição da cobra, a posição da maçã e a pontuação atual. Ademais, é verificado se a cobra está em um loop sem comer maçãs por um determinado número de rodadas. Se isso acontecer, o jogo é encerrado.\n",
        "\n",
        "Após cada rodada, o jogo é renderizado na janela e a taxa de exploração é atualizada. No final do jogo, a pontuação final é exibida e há um pequeno atraso antes de encerrar o jogo completamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PygameSnakeGame:\n",
        "    def __init__(self, q_learning_snake):\n",
        "        self.snake_game = q_learning_snake\n",
        "\n",
        "    def draw_parameters(self, screen):\n",
        "        font = pygame.font.Font(None, 24)\n",
        "        text_margin = 10\n",
        "\n",
        "        score_text = font.render(\n",
        "            f\"Score: {self.snake_game.score}\", True, (255, 255, 255))\n",
        "        screen.blit(\n",
        "            score_text, (WINDOW_SIZE[0] - score_text.get_width() - text_margin, text_margin))\n",
        "\n",
        "        # Adicione outros parâmetros aqui, se desejar\n",
        "\n",
        "    def play(self):\n",
        "        # Inicializando o Pygame e executando o jogo com o modelo treinado\n",
        "        pygame.init()\n",
        "        pygame.display.set_caption(\"Snake Q-Learning\")\n",
        "        screen = pygame.display.set_mode(WINDOW_SIZE)\n",
        "        clock = pygame.time.Clock()\n",
        "        \n",
        "        running = True\n",
        "\n",
        "        while running:\n",
        "            self.snake_game.reset()  # Resetar o jogo antes de começar a jogar\n",
        "\n",
        "            rounds_without_apple = 0\n",
        "            max_rounds_without_apple = 200\n",
        "\n",
        "            while not self.snake_game.game_over:\n",
        "                # Checa se o jogo foi encerrado\n",
        "                for event in pygame.event.get():\n",
        "                    if event.type == pygame.QUIT:\n",
        "                        self.game_over = True\n",
        "                        running = False\n",
        "                \n",
        "                # Escolha na Q-Table e movimento da cobra\n",
        "                state_index = self.snake_game.get_state_index()\n",
        "                action_index = np.argmax(self.snake_game.q_table[state_index])\n",
        "                self.snake_game.change_direction(\n",
        "                    ['up', 'down', 'left', 'right'][action_index])\n",
        "                _, ate_apple = self.snake_game.move()\n",
        "\n",
        "                # Checa se a cobra está em loop sem comer maçãs\n",
        "                if ate_apple:\n",
        "                    rounds_without_apple = 0\n",
        "                else:\n",
        "                    rounds_without_apple += 1\n",
        "                if rounds_without_apple >= max_rounds_without_apple:\n",
        "                    self.snake_game.game_over = True\n",
        "\n",
        "                # Renderizar o jogo na janela\n",
        "                screen.fill(BLACK)\n",
        "                self.draw_parameters(screen)\n",
        "                for segment in self.snake_game.snake:\n",
        "                    pygame.draw.rect(\n",
        "                        screen, GREEN, (segment[0] * GRID_SIZE, segment[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "                pygame.draw.rect(\n",
        "                    screen, RED, (self.snake_game.apple[0] * GRID_SIZE, self.snake_game.apple[1] * GRID_SIZE, GRID_SIZE, GRID_SIZE))\n",
        "                pygame.display.flip()\n",
        "                clock.tick(FPS)\n",
        "\n",
        "            print(f\"Game Over! Score: {self.snake_game.score}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        pygame.quit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dando continuidade, abaixo temos a função responsável por gerar os nossos gráficos e faremos isso através da biblioteca Matplotlib, pois ela desempenha um papel crucial na análise e visualização de comportamentos de modelos baseados em Aprendizado por Reforço (RL - Reinforcement Learning) em Python. Ela oferece uma ampla gama de recursos e ferramentas para a criação de gráficos, gráficos de linha, histogramas, gráficos de dispersão e muito mais. Essas capacidades são essenciais para entender e avaliar o desempenho dos modelos RL e tomar decisões informadas.\n",
        "\n",
        "Uma das principais razões pelas quais o Matplotlib é importante para o Aprendizado por Reforço é sua capacidade de visualizar a função Q ou a política aprendida pelo modelo. Além disso, o Matplotlib é útil para acompanhar o desempenho do modelo ao longo do tempo. Os gráficos de linha podem ser usados para representar métricas relevantes, como a recompensa acumulada ou a taxa de aprendizado, em função do número de episódios ou de iterações de treinamento. Esses gráficos fornecem uma visão clara do progresso do modelo e ajudam a identificar tendências, melhorias ou problemas durante o treinamento.\n",
        "\n",
        "Outro aspecto importante é a análise comparativa de diferentes configurações ou variações do modelo. O Matplotlib permite a criação de múltiplos gráficos lado a lado, o que facilita a comparação de resultados entre diferentes abordagens, parâmetros ou hiperparâmetros. Essa capacidade de visualização comparativa ajuda a identificar qual configuração é mais adequada para um determinado problema.\n",
        "\n",
        "Em resumo, a biblioteca Matplotlib com suas capacidades gráficas avançadas, é possível visualizar a função Q, monitorar o desempenho ao longo do tempo, comparar diferentes configurações do modelo e analisar trajetórias de agentes. Essas visualizações auxiliam na compreensão, avaliação e tomada de decisões informadas sobre o desenvolvimento e ajuste de modelos de RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_plots(rewards, scores, average_scores, epsilons):\n",
        "    # Gráfico de recompensa ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(rewards) + 1), rewards)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Recompensa\")\n",
        "    plt.title(\"Recompensa ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de pontuação ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(scores) + 1), scores)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Pontuação\")\n",
        "    plt.title(\"Pontuação ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de pontuação média ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(average_scores) + 1), average_scores)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Pontuação Média\")\n",
        "    plt.title(\"Pontuação Média ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de epsilon ao longo das épocas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, len(epsilons) + 1), epsilons)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    plt.title(\"Epsilon ao longo das épocas\")\n",
        "    plt.show()\n",
        "\n",
        "    # Salvar os gráficos em um arquivo (opcional)\n",
        "    # plt.savefig(\"q_learning_plots.png\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de Testes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 1:\n",
        "\n",
        "- $ \\varepsilon\\ $ = 1.0\n",
        "\n",
        "- $ min-\\varepsilon\\ $ = 0.01\n",
        "\n",
        "- $ decay-\\varepsilon\\ $ = 0.995\n",
        "\n",
        "- $ \\alpha\\ $ = 0.1\n",
        "\n",
        "- $ \\gamma\\ $ = 0.9\n",
        "\n",
        "\n",
        "**Justificativa:** Com esses parâmetros, podemos notar que estaríamos começando com um valor alto para epsilon (1.0), o que significa que a exploração é inicialmente priorizada. Com o tempo, a probabilidade de explorar diminui gradualmente devido ao fator de decaimento (min_epsilon_decay = 0.995), chegando ao valor mínimo de 0.01 (min_epsilon). Através da definição do alpha como 0.1, nos é permitido um equilíbrio entre a importância dos novos dados e o conhecimento acumulado (taxa de aprendizado). Por fim, o gamma é definido como 0.9 para dar um desconto alto para recompensas futuras, incentivando a maximização de recompensas ao longo do tempo.\n",
        "\n",
        "**Resultados Esperados**: Com base nessas configurações, espera-se que o modelo inicialmente explore amplamente o ambiente, mas conforme o treinamento progride e a taxa de exploração diminui, ele deve começar a tomar decisões mais direcionadas e otimizadas com base nas informações aprendidas na Q-Table. O resultado final será um modelo capaz de jogar a cobrinha de forma eficiente, maximizando a pontuação."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 2:\n",
        "\n",
        "- $ \\varepsilon\\ $ = 0.5\n",
        "\n",
        "- $ min-\\varepsilon\\ $ = 0.001\n",
        "\n",
        "- $ decay-\\varepsilon\\ $ = 0.999\n",
        "\n",
        "- $ \\alpha\\ $ = 0.2\n",
        "\n",
        "- $ \\gamma\\ $ = 0.95\n",
        "\n",
        "**Justificativa:** Neste conjunto, os dados de epsilon, alpha e gamma sofreram alterações. De início, com epsilon sendo igual a 0.5, ainda continuamos priorizando a exploração (apesar de ter um valor menor com relação ao conjunto anterior) e diminuímos ainda mais lentamente a probabilidade de explorar (min_epsilon_decay = 0.999) até o valor mínimo de 0.001 (min_epsilon). O alpha é definido como 0.2, permitindo um pouco mais de importância para os novos dados na atualização da tabela Q. O gamma é definido como 0.95 para dar um desconto um pouco mais alto para recompensas futuras.\n",
        "\n",
        "**Resultados Esperados**: Analisando o conjunto de dados acima, espera-se que o modelo comece com uma exploração moderada do ambiente e, ao longo do tempo, concentre-se cada vez mais em ações otimizadas com base nas informações aprendidas. A taxa de exploração diminuirá gradualmente, mas de forma mais lenta do que nos conjuntos de testes anteriores, permitindo que o modelo continue explorando o ambiente enquanto busca a otimização. O resultado final será um modelo capaz de jogar a cobrinha de forma mais eficiente, buscando maximizar a pontuação considerando recompensas futuras. Teoricamente, deve se mostrar mais eficaz que o primeiro conjunto de testes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Conjunto de Teste 3:\n",
        "\n",
        "- $ \\varepsilon\\ $ = 0.8\n",
        "\n",
        "- $ min-\\varepsilon\\ $ = 0.05\n",
        "\n",
        "- $ decay-\\varepsilon\\ $ = 0.99\n",
        "\n",
        "- $ \\alpha\\ $ = 0.05\n",
        "\n",
        "- $ \\gamma\\ $ = 0.8\n",
        "\n",
        "**Justificativa:** Neste conjunto, começamos com um valor relativamente alto para epsilon (0.8), priorizando a exploração no início, e diminuímos a probabilidade de explorar um pouco mais rapidamente (min_epsilon_decay = 0.99) até o valor mínimo de 0.05 (min_epsilon). O alpha é definido como 0.05, dando menos importância para os novos dados e enfatizando o conhecimento acumulado. O gamma é definido como 0.8 para dar um desconto moderado para recompensas futuras.\n",
        "\n",
        "**Resultados Esperados**: Com exploração alta no início, o agente pode explorar mais intensamente e aprender rapidamente sobre o ambiente. No entanto, a diminuição mais rápida do fator de exploração pode fazer com que o agente se concentre mais nas ações conhecidas e menos nas desconhecidas, levando a um trade-off entre exploração e aproveitamento. Isso pode resultar em um desempenho eficaz para tarefas de curto prazo, mas pode ser menos eficaz para tarefas mais complexas e de longo prazo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Conjunto de Teste 4:\n",
        "\n",
        "- $ \\varepsilon\\ $ = 0.2\n",
        "\n",
        "- $ min-\\varepsilon\\ $ = 0.1\n",
        "\n",
        "- $ decay-\\varepsilon\\ $ = 0.995\n",
        "\n",
        "- $ \\alpha\\ $ = 0.3\n",
        "\n",
        "- $ \\gamma\\ $ = 0.99\n",
        "\n",
        "**Justificativa:** Analisando esse conjunto, temos que, por definição, o parâmetro $ \\varepsilon\\ $ é utilizado para controlar a taxa de exploração e o valor $ \\varepsilon\\ $ = 0.2 indica que o agente irá explorar novas ações em 20% das vezes. Ademais, estamos diminuindo a probabilidade de explorar gradualmente ($ decay-\\varepsilon\\ $) até o valor mínimo de 0.1 ($ min-\\varepsilon\\ $ = 0.1). O alpha, como podemos perceber, é definido como 0.3, dando mais importância para os novos dados na atualização da tabela Q. O $ \\gamma\\ $ é definido como 0.99 para dar um desconto alto para recompensas futuras, incentivando a maximização de recompensas ao longo do tempo.\n",
        "\n",
        "**Resultados Esperados**: Com exploração moderada no início e uma taxa de aprendizado mais alta, o agente pode aprender mais rapidamente com base em novas experiências. O fator de desconto mais alto pode incentivar o agente a considerar recompensas futuras e a otimizar a política global. Espera-se que esse conjunto de parâmetros leve a um aprendizado rápido e a recompensas significativamente altas em tarefas de longo prazo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conjunto de Teste 5:\n",
        "\n",
        "- $ \\varepsilon\\ $ = 0.1\n",
        "\n",
        "- $ min-\\varepsilon\\ $ = 0.001\n",
        "\n",
        "- $ decay-\\varepsilon\\ $ = 0.9999\n",
        "\n",
        "- $ \\alpha\\ $ = 0.1\n",
        "\n",
        "- $ \\gamma\\ $ = 0.95\n",
        "\n",
        "**Justificativa:** O conjunto de parâmetros apresentado acima indica uma configuração de aprendizado mais voltada para a exploração do ambiente e uma taxa de aprendizado mais conservadora, pois diminuímos muito lentamente a probabilidade de explorar ($ decay-\\varepsilon\\ $ = 0.9999) até o valor mínimo de 0.001 ($ min-\\varepsilon\\ $). O alpha é definido como 0.1, dando um equilíbrio entre a importância dos novos dados e o conhecimento acumulado na atualização da tabela Q. O gamma é definido como 0.95 para dar um desconto um pouco mais alto para recompensas futuras.\n",
        "\n",
        "**Resultados Esperados**: Com base nessas configurações, espera-se que o modelo comece com uma exploração moderada do ambiente e, ao longo do tempo, se concentre cada vez mais em ações otimizadas com base nas informações aprendidas. A taxa de exploração diminuirá gradualmente, permitindo que o modelo refine suas ações à medida que ganha mais conhecimento sobre o ambiente e busca a otimização. O resultado final será um modelo capaz de jogar a cobrinha de forma mais eficiente, maximizando a pontuação considerando tanto as recompensas imediatas quanto as recompensas futuras."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos resultados obtidos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A análise dos resultados obtidos a partir dos testes realizados revela insights importantes sobre o desempenho e a eficácia do modelo. Ao examinar as métricas, como a média de pontuações e a progressão ao longo das épocas, é possível identificar padrões e tendências que nos ajudam a compreender o comportamento do modelo de aprendizado de máquina. Essa análise permite ajustar os parâmetros e estratégias de treinamento para otimizar o desempenho e aprimorar a capacidade do modelo de tomar decisões e obter resultados mais eficazes. Com base nessa análise, é possível tomar medidas para melhorar o modelo e obter um desempenho mais satisfatório em tarefas futuras. Agora, buscarei destrincar, conjunto a conjunto, os resultados obtidos. Assim sendo, abaixo segue as análises individuais de cada um:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*  **Conjunto 1:** \n",
        "\n",
        "Os testes realizados com os parâmetros fornecidos mostraram que o modelo atingiu pontuações médias consideráveis, embora não tenha alcançado os melhores resultados possíveis. A taxa de exploração inicial alta permitiu uma descoberta rápida de estratégias eficazes, e a redução gradual da taxa de exploração levou a um foco maior em ações otimizadas. No entanto, a relação entre recompensas e épocas apresentou um desempenho um pouco baixo, sugerindo a necessidade de ajustes nos parâmetros ou nas recompensas para obter resultados mais satisfatórios.\n",
        "\n",
        "*  **Conjunto 2:** \n",
        "\n",
        "Os resultados dos testes com os parâmetros fornecidos mostraram que o modelo apresentou uma taxa de progressão gradual em relação à média de pontuações. Embora a melhoria tenha sido lenta, os gráficos indicaram uma tendência crescente ao longo das épocas, sugerindo que, com um número significativo de iterações, o modelo tem potencial para continuar aprendendo e alcançar pontuações mais elevadas. Essa trajetória ascendente indica que os parâmetros utilizados podem ser adequados para estimular um aprendizado consistente e progressivo do modelo de Aprendizado por Reforço.\n",
        "\n",
        "*  **Conjunto 3:** \n",
        "\n",
        "Os resultados dos testes indicam que o modelo foi capaz de aprender rapidamente, alcançando uma pontuação satisfatória em um curto período de tempo. No entanto, o desempenho do modelo se estabilizou após um certo número de épocas, indicando uma limitação em melhorar ainda mais sua performance. Isso sugere que os parâmetros utilizados podem ter sido eficazes em um estágio inicial de aprendizado, mas não foram capazes de impulsionar um progresso contínuo ao longo do tempo.\n",
        "\n",
        "*  **Conjunto 4:** \n",
        "\n",
        "Os resultados dos testes indicam que o modelo inicialmente teve um progresso rápido de aprendizado, mostrando uma melhoria significativa em sua pontuação. No entanto, ao longo do tempo, o desempenho do modelo estagnou e começou a declinar, resultando em uma média de pontuação baixa em comparação com outros modelos. Isso sugere que os parâmetros utilizados podem ter sido eficazes em um estágio inicial, mas não foram capazes de sustentar um desempenho consistente e de alto nível ao longo do tempo.\n",
        "\n",
        "*  **Conjunto 5:**\n",
        "\n",
        "Os resultados dos testes indicam que o modelo teve um início promissor, com uma taxa de progressão rápida na média de pontuações. No entanto, ele logo atingiu um ponto de estagnação, com uma diminuição na taxa de crescimento e desempenho geral. Isso sugere que os parâmetros utilizados foram eficazes em um estágio inicial, mas não foram capazes de manter um desempenho consistente e eficiente ao longo do tempo. Como resultado, o modelo não alcançou pontuações significativamente altas e apresentou limitações em seu aprendizado."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinando o nosso modelo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo, temos o código responsável pelo treinamento de nosso modelo. É por meio deste que o modelo é treinado por meio de repetidas interações com o ambiente, buscando otimizar suas políticas de ação para maximizar a recompensa cumulativa ao longo do tempo. O treinamento de um modelo de Aprendizado por Reforço é um desafio complexo, exigindo a adequada definição de parâmetros e a implementação de estratégias de exploração e exploração. Assim sendo, abaixo o mesmo se encontra, porém não estando executado, já que isso demandaria tempo e fizemos as análises através de treinamentos individualizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizando o treinamento do modelo\n",
        "snake_game = QLearningSnake(width=GRID_WIDTH, height=GRID_HEIGHT)\n",
        "rewards, scores, average_scores, epsilons = snake_game.train(num_episodes=1000000)\n",
        "\n",
        "#Salvando modelo\n",
        "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "model_filename = f'output/snake_{current_datetime}.pkl'\n",
        "snake_game.save_model(model_filename)\n",
        "\n",
        "# Gerando os gráficos após o treinamento\n",
        "generate_plots(rewards, scores, average_scores, epsilons)\n",
        "\n",
        "# Salvando os resultados obtidos em um arquivo CSV\n",
        "results_filename = f'output/results_{current_datetime}.csv'\n",
        "snake_game.save_results(results_filename, rewards,\n",
        "                        scores, average_scores, epsilons)\n",
        "\n",
        "# Inicializando o Pygame e executando o jogo com o modelo treinado\n",
        "# game = PygameSnakeGame(snake_game)\n",
        "# game.play()\n",
        "\n",
        "# Carregando um modelo treinado\n",
        "# snake_game = QLearningSnake(width=GRID_WIDTH, height=GRID_HEIGHT)\n",
        "# snake_game.load_model('output/snake_YYYYMMDDHHMMSS.pkl')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxQT1vP2U4U"
      },
      "source": [
        "### Agradecimentos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oej9nzPsS6l"
      },
      "source": [
        "Gostaria de utilizar esse espaço para ressaltar meus agradecimentos aos seguintes professores:\n",
        "\n",
        "*   Hugo Tremonte de Carvalho\n",
        "*   João Antônio Recio da Paixão\n",
        "*   João Carlos Pereira da Silva\n",
        "\n",
        "Por fim, esperamos termos sido capazes de conseguir articular bem os conceitos, pois os mesmos não são fáceis. Dessa forma, viemos por meio deste terminar com uma breve citação:\n",
        "\n",
        "# ***“A esperança é uma coisa boa, talvez a melhor de todas, e nada que é bom, deve morrer” - King, Stephen***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMHuTboOfoI"
      },
      "source": [
        "# Referências Bibliográficas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ20lrJjOf8s"
      },
      "source": [
        "https://www.youtube.com/watch?v=je0DdS0oIZk\n",
        "\n",
        "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/\n",
        "\n",
        "https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c\n",
        "\n",
        "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#\n",
        "\n",
        "https://www.deeplearningbook.com.br/componentes-do-aprendizado-por-reforco-reinforcement-learning/\n",
        "\n",
        "https://www.deeplearningbook.com.br/distribuicoes-de-probabilidade-redes-neurais-e-reinforcement-learning/\n",
        "\n",
        "http://monografias.ice.ufjf.br/tcc-web/exibePdf?id=33#:~:text=Processos%20de%20decis%C3%A3o%20de%20Markov%20(MDP%20%E2%80%93%20Markov%20Decision%20Process),a%C3%A7%C3%A3o%20aplicada%20sobre%20esse%20estado.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
